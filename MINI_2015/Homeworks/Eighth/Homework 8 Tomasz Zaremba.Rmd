---
title: "Homework #8"
author: "Tomasz Zaremba"
date: "2015-12-03"
output: 
  html_document:
    toc: TRUE
---

# Introduction

In the previous project 3 teams used Random Forest for classification on the same data set and got very different results. In this homework I'll try to find out why.

# Preparing data

Loading the set.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
library(caret)
library(ROCR)

australian = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat",
        sep=" ",header=F,fill=FALSE,strip.white=T)

australian$V15 <- factor(ifelse(australian$V15 == 0, "No", "Yes"))
```

# Performance of classifiers

Tomasz Kozicki, Marcel Szczêsny - cross validation
```{r, warning=FALSE, message=FALSE, cache=TRUE}
train_control <- trainControl(method="cv", number=10);
model <- train(V15 ~ V8 + V7 + V10 + V14 + V3, method="rf", data=australian, trControl=train_control, preProcess=c('scale', 'center'));
info = confusionMatrix(australian$V15, predict(model, australian))$overall

accuracy1CV = round(info*100,2)
accuracy1CV
```

Tomasz Kozicki, Marcel Szczêsny - training/testing
```{r, warning=FALSE, message=FALSE, cache=TRUE}

indxTrain <- createDataPartition(y = australian$V15, p = 0.8)
australianDataSetTrain<- australian[indxTrain$Resample1,]
australianDataSetTest <- australian[-indxTrain$Resample1,]

model <- train(V15 ~ V8 + V7 + V10 + V14 + V3, method="rf", data=australianDataSetTrain, preProcess=c('scale', 'center'));
info = confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall

accuracy1TT = round(info*100,2)
accuracy1TT
```

We can easily see that the same classifier gave almost 100% accuracy using cross validation (k = 10) and less than 90% for division into training (80%) and testing (20%) set. 

Karolina Kwasiborska, Tomasz Zaremba, Ziad Al Bkhetan - cross validation
```{r, warning=FALSE, message=FALSE, cache=TRUE}
train_control <- trainControl(method="cv", number=10);
model <- train(V15 ~ V8 + V14 + V10 + V5 + V7 + V13 + V9, method="rf", data=australian, trControl=train_control, preProcess=c('scale', 'center'));
info = confusionMatrix(australian$V15, predict(model, australian))$overall

accuracy2CV = round(info*100,2)
accuracy2CV
```

Karolina Kwasiborska, Tomasz Zaremba, Ziad Al Bkhetan - training/testing
```{r, warning=FALSE, message=FALSE, cache=TRUE}

model <- train(V15 ~ V8 + V14 + V10 + V5 + V7 + V13 + V9, method="rf", data=australianDataSetTrain, preProcess=c('scale', 'center'));
info = confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall

accuracy2TT = round(info*100,2)
accuracy2TT
```

Again, the same classifier (more variables than in the previous case) gave almost 100% accuracy using cross validation (k = 10) and less than 90% for division into training (80%) and testing (20%) set. The bigger number of variables doesn't seem to influence the results much.

Karolina Kwasiborska, Tomasz Zaremba, Ziad Al Bkhetan - training/testing 90/10
```{r, warning=FALSE, message=FALSE, cache=TRUE}

indxTrain <- createDataPartition(y = australian$V15, p = 0.9)
australianDataSetTrain<- australian[indxTrain$Resample1,]
australianDataSetTest <- australian[-indxTrain$Resample1,]


model <- train(V15 ~ V8 + V14 + V10 + V5 + V7 + V13 + V9, method="rf", data=australianDataSetTrain, preProcess=c('scale', 'center'));
info = confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall

accuracy2TT2 = round(info*100,2)
accuracy2TT2
```

The results for 90/10 proportions are similarly bad.

Karolina Kwasiborska, Tomasz Zaremba, Ziad Al Bkhetan - training/testing 70/30
```{r, warning=FALSE, message=FALSE, cache=TRUE}

indxTrain <- createDataPartition(y = australian$V15, p = 0.7)
australianDataSetTrain<- australian[indxTrain$Resample1,]
australianDataSetTest <- australian[-indxTrain$Resample1,]

model <- train(V15 ~ V8 + V14 + V10 + V5 + V7 + V13 + V9, method="rf", data=australianDataSetTrain, preProcess=c('scale', 'center'));
info = confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall

accuracy2TT3 = round(info*100,2)
accuracy2TT3
```

The results for 70/30 are similarly bad.

Margareta Kusan, Neven Piculjan - cross validation
```{r, warning=FALSE, message=FALSE, cache=TRUE}

indxTrain <- createDataPartition(y = australian$V15, p = 0.8)
australianDataSetTrain<- australian[indxTrain$Resample1,]
australianDataSetTest <- australian[-indxTrain$Resample1,]

train_control <- trainControl(method="cv", number=10);
model <- train(V15 ~ V8 + V10 + V7, method="rf", data=australian, trControl=train_control, preProcess=c('scale', 'center'));
info = confusionMatrix(australian$V15, predict(model, australian))$overall

accuracy3CV = round(info*100,2)
accuracy3CV
```

Margareta Kusan, Neven Piculjan - training/testing
```{r, warning=FALSE, message=FALSE, cache=TRUE}

model <- train(V15 ~ V8 + V10 + V7, method="rf", data=australianDataSetTrain, preProcess=c('scale', 'center'));
info = confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall

accuracy3TT = round(info*100,2)
accuracy3TT
```

The results in the third group are slighly worse because they used fewer variables (not enough) but again CV gives better results than dividing into training and testing set.

#Conclusions
We can easily observe that dividing the set into training (80%) and testing (20%) and measuring the accuracy by ourselves gives much worse results than simply using cross validation. Increasing the size of the training set to 90% or decreasing to 70% doesn't influence the results significantly. We might suspect that the division into training and testing sets is very unlucky and some of the observations in the testing set seem much different from the ones in the training set and for this reason the classifier is not able to classify them properly. The other factor which influences the results is the choice of variables. As we can see, variables V8, V10 and V7 are not enough to describe the data in a way that will allow for proper classification. Greater number of variables chosen by groups 1 and 2 yielded much better results.
