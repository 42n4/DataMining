---
title: "Homework 8"
author: "Marcel Sz"
date: "3 grudnia 2015"
output: html_document
---

#Homework Description

Check the purpose of difference while computing Random Forest accuracy for the australian set.

#Loading data

```{r, warning=FALSE, message=FALSE}
library(caret)
library(ROCR)

australian = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat",
        sep=" ",header=F,fill=FALSE,strip.white=T)
originalaustralianDataSet <-australian
australian$V15 <- factor(ifelse(australian$V15 == 0, "No", "Yes"))
```

# First Group (Marcel Sz, Tomasz K)

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)
rf <- train(V15 ~ ., data=australian, trControl=train_control, method="rf")
importance <- varImp(rf, scale=FALSE)
print(importance)
plot(importance)
```
The best features are: V8, V7, V10, V14.
The worst features are: V1, V11, V12, V4.

##Training classifier using all variables.
```{r, warning=FALSE, message=FALSE}
classifiers <- c('rf')
info <- sapply(classifiers, function (met) {
  train_control <- trainControl(method="cv", number=10)
  model <- train(V15 ~ ., method=met, data=australian, trControl=train_control, preProcess=c('scale', 'center'))
  confusionMatrix(australian$V15, predict(model, australian))$overall
})
accuracies = round(info*100,2)
accuracies

```

##Training classifier with the best features.

```{r, warning=FALSE, message=FALSE}
classifiers <- c('rf')
info <- sapply(classifiers, function (met) {
  train_control <- trainControl(method="cv", number=10)
  model <- train(V15 ~ V8 + V7 + V10 + V14 + V3, method=met, data=australian, trControl=train_control, preProcess=c('scale', 'center'))
  confusionMatrix(australian$V15, predict(model, australian))$overall
})
accuracy1 = round(info*100,2)
accuracy1
```

##AUC of classifiers

```{r, warning=FALSE, message=FALSE}
aucs <- c()
for (cl in classifiers) {
  train_control <- trainControl(method="cv", number=10)
  model <- train(V15 ~ .,  method=cl, data=australian, trControl=trainControl(classProbs=TRUE))
  prob <- predict(model, australian, type = "prob")[,2]

  fit.pred = prediction(prob, australian$V15)
  fit.perf = performance(fit.pred,"auc")
  aucs <- c(aucs, fit.perf@y.values[[1]])
}
  aucs <- as.numeric(aucs)
  names(aucs) <- classifiers
  aucs <- sort(aucs, decreasing = TRUE)
  aucs
```

#Second group (Tomasz Z, Karolina K)

```{r, warning=FALSE, message=FALSE}
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(MASS)
library(Hmisc)
library(ROCR)
library(pls)
library(klaR)
  
  
australianZaremba <-originalaustralianDataSet
australianZaremba$V15 <- factor(ifelse(australianZaremba$V15 == 0, "One", "Zero"))
#partitioning
indxTrain <- createDataPartition(y = australianZaremba$V15, p = 0.8)
australianDataSetTrain<- australianZaremba[indxTrain$Resample1,]
australianDataSetTest <- australianZaremba[-indxTrain$Resample1,]
# Data normalization
normalize <- function(x) {
  scale(x)
}

australianDataSetNormalized <- as.data.frame(lapply(australianZaremba[, 1:14], normalize))
australianDataSetNormalized$V15 <-australianZaremba$V15
# Training and Testing Datasets
indxTrain <- createDataPartition(y = australianDataSetNormalized$V15, p = 0.8)
australianDataSetNormalizedTrain<- australianDataSetNormalized[indxTrain$Resample1,]
australianDataSetNormalizedTest <- australianDataSetNormalized[-indxTrain$Resample1,]

# Data Categorization
aust_equal_size <- originalaustralianDataSet
for (i in c(2, 3, 4, 5, 6, 7, 10, 12, 13, 14)) {
  aust_equal_size[,i]<- cut2(australianZaremba[,i], g=3)
}
aust_equal_size$V15 <- factor(ifelse(aust_equal_size$V15 == 0, "One", "Zero")) 
aust_equal_size$V1 <-factor(aust_equal_size$V1)
aust_equal_size$V8 <-factor(aust_equal_size$V8)
aust_equal_size$V9 <-factor(aust_equal_size$V9)
aust_equal_size$V11 <-factor(aust_equal_size$V11)
# Partitioning 
indxTrain <- createDataPartition(y = aust_equal_size$V15, p = 0.8)
aust_equal_sizeTrain<- aust_equal_size[indxTrain$Resample1,]
aust_equal_sizeTest <- aust_equal_size[-indxTrain$Resample1,]
```

##Determining important variables
```{r , cache=FALSE, warning=FALSE, message=FALSE}
allVariablesForest <- randomForest(V15 ~ ., data = australianZaremba, importance = TRUE, na.action = na.omit)
varImpPlot(allVariablesForest)
##Important V8 + V14 + V10 + V5 + V7 + V13 + V9.
```

## Classifiers Training And Testing

```{r , warning=FALSE, message=FALSE, results="hide"}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=T, savePredictions = T)
classifiersCount = 1
counter = 1
outMat<-matrix(list(), nrow=classifiersCount, ncol=2)
conf_mat <-matrix(list(), nrow=classifiersCount, ncol=1)
rocMat<-matrix(list(), nrow=classifiersCount, ncol=2)

for(classfierType in c('rf'))
{
          trainDS <- australianDataSetTrain
          testDS <- australianDataSetTest
  
  currModel<- train(V15 ~ V8 + V14 + V10 + V5 + V7 + V13 + V9, data=trainDS, method=classfierType, trControl=control)
  predTab <- predict(currModel, testDS)
  outMat[[counter, 1]] <- table(true = testDS$V15, predicted = predTab)
  # Accuracy
  outMat[[counter, 2]] <- sum(diag(outMat[[counter, 1]])) / sum(outMat[[counter, 1]])
  # Confusion matrix
  conf_mat[[counter, 1]] <- confusionMatrix(predTab, testDS$V15)
  # ROC and AUC
  
  predTabPro <- predict(currModel, newdata=testDS, type="prob")[,2] 
  pred <- prediction(predTabPro, testDS$V15)
  rocMat[[counter, 1]] <- performance(pred, "tpr", "fpr")
  # Area Under Curve
  rocMat[[counter, 2]] <- performance(pred,"auc")

  counter= counter + 1
}

```

```{r}
vals <- rep(0.00, times = 24)
for(counter in 1:classifiersCount)
{
  vals[2 * counter - 1] = outMat[[counter, 2]]
  vals[2 * counter ] = rocMat[[counter, 2]]@y.values[[1]]
}

mesTab <- matrix(vals, ncol=2, nrow = 1, byrow = TRUE)
colnames(mesTab) <- c("Accuracy",  "AUC")
rownames(mesTab) <- c("Random Forests")
mesTab
```

# Nevenp and Margaret's Group

```{r}
australianNeven <- originalaustralianDataSet
australianNeven$V15 <- factor(ifelse(australianNeven$V15 > 0, "Yes", "No")) 
```

```{r, warning=FALSE}
set.seed(1313)
indxTrain <- createDataPartition(y = australianNeven$V15, p = 0.75)
str(indxTrain)

australianTrain <- australianNeven[indxTrain$Resample1,]
australianTest <- australianNeven[-indxTrain$Resample1,]
```

## Measuring accuracy for Random Forest with all variables
```{r, warning=FALSE}
classifiers <- c('rf')
all_accuracy_random_forest <- sapply(classifiers, function (met) {
  modelFit<- train(V15~., method=met, data=australianTrain)
  confusionMatrix(australianTest$V15, predict(modelFit, australianTest))$overall
})

all_accuracy_random_forest <- t(round(all_accuracy_random_forest*100,2))
all_accuracy_random_forest[1]
```

##Training classifier using all variables

```{r, warning=FALSE}
library(ROCR)
all_auc_rf <- c()
for (i in classifiers) {
  ffit <- train(V15~.,  method=i, data=australianNeven, trControl=trainControl(classProbs=TRUE))
  prob <- predict(ffit, australianNeven, type = "prob")[,2]

  fit.pred = prediction(prob, australianNeven$V15)
  fit.perf = performance(fit.pred,"auc")
  all_auc_rf <- c(all_auc_rf, fit.perf@y.values[[1]])
}
all_auc_rf[1]
```

## Measuring variable importance for chosen classifiers 

```{r, warning=FALSE}
train_control <- trainControl(method="cv", number=10)
for (i in classifiers){
  model <- train(V15~., data=australianNeven, trControl=train_control, method=i)
  varImp(model, scale = TRUE)
}
```
They chose V8, V10 and V7 as the best classifiers


## Measuring accuracy and AUC RandomForest with three most important variables
```{r, warning=FALSE}
# Accuracy and AUC for rf with only three most important variables
rf_modelFit<- train(V15~V8+V10+V7, method='rf', data=australianTrain)
rf_acc <- confusionMatrix(australianTest$V15, predict(rf_modelFit, australianTest))$overall
rf_acc <- t(round(rf_acc*100,2))[1]
rf_acc[1]
```

```{r}
ffit <- train(V15~V8+V10+V7,  method='rf', data=australianNeven, trControl=trainControl(classProbs=TRUE))
prob <- predict(ffit, australianNeven, type = "prob")[,2]
fit.pred = prediction(prob, australianNeven$V15)
fit.perf = performance(fit.pred,"auc")
rf_auc <- fit.perf@y.values[[1]]
rf_auc[1]
```


#Remarks
Accuracy of Random Forest classifier is the best one in our group, because we are using original dataset as a training set and testing set. Which will cause better result. The reason behind this is that classifier 'has better knowledge' about the testing dataset.
Applying original models in other group projects as a training set and testing set will result in similar results. Increasing the number of features (from three to five) in the 3rd project does not give significant change in accuracy.

Proof (by refactoring our project, which uses different training and testing set for classifier - not original one).

All variables
```{r}
train_control <- trainControl(method="cv", number=10)
model <- train(V15 ~ ., method='rf', data=australianTrain, trControl=train_control, preProcess=c('scale', 'center'))
remarkResultAll <- confusionMatrix(australianTest$V15, predict(model, australianTest))$overall
remarkResultAll <- t(round(remarkResultAll*100,2))[1]
remarkResultAll[1]
```

Important features only
```{r}
train_control <- trainControl(method="cv", number=10)
model <- train(V15 ~ V8 + V7 + V10 + V14, method='rf', data=australianTrain, trControl=train_control, preProcess=c('scale', 'center'))
remarkResult <- confusionMatrix(australianTest$V15, predict(model, australianTest))$overall
remarkResult <- t(round(remarkResult*100,2))[1]
remarkResult[1]
```