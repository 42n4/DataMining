---
title: "Homework 8"
author: "Ziad Al Bkhetan"
date: "December 3, 2015"
output: 
  html_document:
    toc : TRUE
---

#Introduction
In the last phase of the first project three different projects were submited, and for the random forests classifier we got two different accuracy values:
<br>Team 1 (Tomasz K, Marcel Sz) accuracy about 0.97
<br>Team 2 (Karolina Kwasiborska, Tomasz Zaremba, Ziad Al Bkhetan) accuracy about 0.87
<br>Team 3 (Margareta Kusan, Neven Piculjan) accuracy about 0.86

so in this homework , I have invistigated in the teams scripts to find the reason, at the beging I noticed that the first team who has the best accuracy used the full data set for trainig and testing, and the same for the third team, but the the second team used tarining and testing dataset.
<br><br> Normaly if you used the same dataset for training and testing you will get better result because the classifier will know more about the testing dataset, so to confirm if this is the reason for the differences in accuracy I applied the same models in the projects on the same dataset for training and testing, and I got the same results for the first and second team accuracy = 0.97, but worse result for the third team. 
<br>I checked their model and I found that they used three features for the classifier and it is a good reason to explain why they got bad results because in this dataset the three features they used werenot able to describe the dataset very well and when I changed this feature dataset I got the same results.

<br> in the description below you  will find the results for these classifiers, the first script using the same data set, and the second using different dataset for testing and training.


# Data Loading
```{r}
australian = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat",
                        sep=" ",header=F,fill=FALSE,strip.white=T)

```

# Data Preparation
For the target attribute we will create categorical variable with two different values Yes and No.
and also I will divide the dataset into two different sets the first one for classifier training while the second is for the classifier testing.
<br>* partion percentage is 80:20
<br>* target attribute to maintain good distribution "V15"

 
```{r , warning=FALSE}
library(caret)
library(randomForest)
library(MASS)

australian$V15 <- factor(ifelse(australian$V15 == 0, "No", "Yes"))

indxTrain <- createDataPartition(y = australian$V15, p = 0.8)
australianDataSetTrain<- australian[indxTrain$Resample1,]
australianDataSetTest <- australian[-indxTrain$Resample1,]

```


# Compare Classifers Using The Full Dataset for Training and Testing
in the scripts all classifiers used the same dataset for testing and training, and all of them got the same results except the first model for the third team because they used three features fot the model, and when I used the full features datset I got the same results aproximatly, accuracy = 0.97.
```{r , warning=FALSE}
#First Team 
# first model using all features
train_control <- trainControl(method="cv", number=10)
model <- train(V15 ~ ., method='rf', data=australian, trControl=train_control, preProcess=c('scale', 'center'))
confusionMatrix(australian$V15, predict(model, australian))$overall

# second model using the important features

model <- train(V15 ~ V8 + V7 + V10 + V14 + V3, method='rf', data=australian, trControl=train_control, preProcess=c('scale', 'center'))
confusionMatrix(australian$V15, predict(model, australian))$overall



#The Second Team one model using the important features
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=T, savePredictions = T)
currModel<- train(V15 ~ V8 + V14 + V10 + V5 +V7 + V13 + V9, data=australian, method='rf', trControl=control)
confusionMatrix(australian$V15, predict(model, australian))$overall



#the Third Team  
# the first model using three features
# Notice for this model the result was bad about 0.86
ffit <- train(V15~V8+V10+V7,  method='rf', data=australian, trControl=trainControl(classProbs=TRUE))
confusionMatrix(australian$V15, predict(ffit, australian))$overall

# the second model using all features
ffit <- train(V15~.,  method='rf', data=australian, trControl=trainControl(classProbs=TRUE))
confusionMatrix(australian$V15, predict(ffit, australian))$overall
  
```


# Compare Classifers Using The Training Dataset for Training and Testing Dataset for Testing
In this Script all the classifiers used different dataset for ttraining and testing, and all of them get the same result approximatly, accuracy = 0.87
```{r , warning=FALSE}
#First Team 
# first model using all features
train_control <- trainControl(method="cv", number=10)
model <- train(V15 ~ ., method='rf', data=australianDataSetTrain, trControl=train_control, preProcess=c('scale', 'center'))
confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall

# second model using the important features

model <- train(V15 ~ V8 + V7 + V10 + V14 + V3, method='rf', data=australianDataSetTrain, trControl=train_control, preProcess=c('scale', 'center'))
confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall



#The Second Team one model using the important features
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=T, savePredictions = T)
currModel<- train(V15 ~ V8 + V14 + V10 + V5 +V7 + V13 + V9, data=australianDataSetTrain, method='rf', trControl=control)
confusionMatrix(australianDataSetTest$V15, predict(model, australianDataSetTest))$overall



#the Third Team  
# the first model using three features
ffit <- train(V15~V8+V10+V7,  method='rf', data=australianDataSetTrain, trControl=trainControl(classProbs=TRUE))
confusionMatrix(australianDataSetTest$V15, predict(ffit, australianDataSetTest))$overall

# the second model using all features
ffit <- train(V15~.,  method='rf', data=australianDataSetTrain, trControl=trainControl(classProbs=TRUE))
confusionMatrix(australianDataSetTest$V15, predict(ffit, australianDataSetTest))$overall
  
```

# Remarks
as the scripts and the result shows:
<br>when the same dataset used all classifiers were the same, except the third team classifier that used three features when we used the same dataset for testing and training, 
<br>When we used different datasets one for training and one for testing also all the classifiers were the same even the third team classifier that used three features, 
so I can summarize that all classifier are approximatly the same except the third team classifier that used three features , but they didn't use the same dataset for evaluation.