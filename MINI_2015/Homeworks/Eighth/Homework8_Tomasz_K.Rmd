---
title: "Homework 8"
author: "Tomasz K"
date: "03-12-2015"
output: html_document
---

# Homework Description

Check why groups got different accuracy result when using Random Forest classifier for the australian dataset.

# Loading data

```{r, warning=FALSE, message=FALSE}
library(caret)
library(randomForest)

australian = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat",
        sep=" ",header=F,fill=FALSE,strip.white=T)
australian$V15 <- factor(ifelse(australian$V15 == 0, "False", "True"))

set.seed(1313)
indxTrain <- createDataPartition(y = australian$V15, p = 0.75)
str(indxTrain)

australianTrain <- australian[indxTrain$Resample1,]
australianTest <- australian[-indxTrain$Resample1,]
```

# Approach #1 (Tomasz K., Marcel Sz.)

Our solution was using the same set for training and retrieving confusion matrix. We relied on k-folds crossvalidation to make sure that the model is propery built.

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)
rf1 <- train(V15 ~ ., data=australian, trControl=train_control, method="rf", preProcess=c('scale', 'center'))
cm1 <- confusionMatrix(australian$V15, predict(rf1, australian))
cm1
```

Resulting accuracy ~0.98

# Approach #2 (Neven P., Margareta K.)

Here solution authors have split the set into training and testing parts, and didn't apply cross-validation.

```{r, warning=FALSE, message=FALSE}
rf2 <- train(V15 ~ ., data=australianTrain, method="rf")
cm2 <- confusionMatrix(australianTest$V15, predict(rf2, australianTest))
cm2
```

Resulting accuracy ~0.86

# Approach #3 (Tomasz Z., Karolina K., Ziad)

Here also the set was split into training and testing parts. Additionaly custom repeated k-folds crossvalidation was applied.

```{r, warning=FALSE, message=FALSE}
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=T, savePredictions = T)
rf3 <- train(V15 ~ ., data=australianTrain, method="rf", trControl=control)
cm3 <- confusionMatrix(australianTest$V15, predict(rf3, australianTest))
cm3
```

Resulting accuracy ~0.86

#Conclusions

We can clearly see that the application of a model fitted on one set and building predictions on the same set yields high accuracy because it's the same set. Building model on training set and applying it on testing set seems also sensible. This ~10% difference in accuracy between approach #1 and #2/#3 is resulting from the fact that on testing set it is almost always harder to make predictions than on the set it was trained on. When we are not using crossvalidation, such approach with testing set seems good, but in case of crossvalidation it is largely redundant. 