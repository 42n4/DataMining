---
title: "Homework 6"
author: "Marcel Sz"
date: "2015-11-18"
output: 
  html_document:
  toc: TRUE
---
# Homework 6
## Homework description 
  
  Use the k-fold cross validation to assess the performance of lda/qda on the wines dataset.

#Loading dataset and necessary library
```{r, cache=TRUE}
wines <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", sep=";", header=TRUE)
table(wines$quality)

library(MASS)
```

#Categorizing each variable (Supplying categorical variables)
```{r}
winesb <- wines
winesb$quality <- factor(ifelse(wines$quality > 5, "good", "bad")) 
table(wines$quality)
```

#K-Fold cross validation
```{r}
library(caret)
train_control <- trainControl(method="cv", number=10)
```

## Training the model
Done in more automatic way (by using 'Caret' package)
```{r,warning=FALSE, message=FALSE}
model <- train(quality~., data=winesb, trControl=train_control, method="nb")
```

## Defining predictions
```{r, warning=FALSE}
predictions <- predict(model, winesb)
```
## Summarizing results
```{r}
confusionMatrix(predictions, winesb$quality)
```
It is possible to increase the number of classes to the sample size so that it leads to 'one-leave-out validation'.

# Linear discriminant analysis (LDA) 
It groups follow Gaussian distribution which has the same structure of variance-covarianceand it has  a difference in means.

```{r, cache=TRUE, warning=FALSE, message=FALSE}
lda.model <- lda(quality~., data=winesb)
lda.model

lda.pred <- predict(lda.model, winesb)
names(lda.pred)

table(predicted = lda.pred$class, real = winesb$quality)
```

#QDA and LDA by using 'Caret' library (more universal)

```{r}
mat <- sapply(c('lda', 'qda'), function (met) {
  modelFit<- train(quality~., method=met,preProcess=c('scale', 'center'), data=winesb, trControl=train_control)
  confusionMatrix(winesb$quality, predict(modelFit, winesb))$overall
})

round(mat*100,2)
```
#Remarks
Accuracy of Lda is better, than Qda (74.55 vs 74.42)
