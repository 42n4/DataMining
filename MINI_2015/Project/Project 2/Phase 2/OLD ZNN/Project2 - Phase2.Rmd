---
title: "Project2 Phase 2"
author: "Ziad Al Bkhetan, Neven Piculjan, Naveen Mupparapu"
date: "January 7, 2016"
output: 
  html_document:
    toc : TRUE
---


# Internet of Things - Introduction

In the second phase we should cluster the records in the dataset, then try to charactrize these clusters and find some intersting patterns for these visits.

We Assumed That the visit is the duration between the minimum date and maximum date for the same visitor in the same station, in the same day.

we used a dataset contains 500000 records, and we analysed it, just to make the performance faster.


# Data Loading And Cleaning
In this step we will prepare the data set to start analysis phase, we assumed that the maybe the dataset is not sorted, so we sort it based on the visitor and date.

We removed the records when the visitor is -1, because they are incorrect data.

```{r, warning=FALSE, message=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(cluster)
library(caret)
load('D:/MSc Computer science and informatics/3rd semester/Data Mining - Advances/Lab Projects/Project 2/SmallLogs_n.rda')

# Cleaning
orderedData = arrange(orderedData, visitor, station, date)
orderedData = filter(orderedData, visitor != -1)

```

# Prepare The Final DataSet
in this step we will fetch all needed data for analysis and visualization, and the most important for us are: Visitor ID, Station, Consumed Time, Day, and Hour.

we grouped all the data based on the visitor and the station and the visit day, because each visit should be in one day, and the card is valid for one day.

After that we calculated the visit duration as the difference between the minimum and maximum time for each visitor in each station in the same day, this difference is calculated in minutes.

For Clustering, We decided to cluster these visits based on two different datasets

The First one is a normalized dataset, and we will use these variables to calculate the distances

- the consumed time in each visit
- Start Time


We Normalized the Data, to reduce the effect of variables values range when calculate the distnces.

The second Dataset, using the interactions in each visit, so we put one if the page was visited, and zero if not, then we calcualte the ditances based on the similar visited pages.


```{r, warning=FALSE, message=FALSE}

orderedDataFinal = orderedData %>% 
  group_by(Visitor=visitor, dat=format(date, format="%Y:%m:%d") , Station=station) %>%
  summarise(
    start_time = min(date),
    send_time = max(date),
    Cons_time = difftime(max(date), min(date),units='mins'),
    WDay = as.POSIXlt(min(date))$wday,
    THour = as.POSIXlt(min(date))$hour
    
  )  
finalData <- orderedDataFinal
finalData$Cons_time <- as.numeric(finalData$Cons_time)
finalData$Station <- factor(finalData$Station)
finalData <- filter(finalData, Cons_time > 1)

finalData <- filter(finalData, Cons_time < 5)
indxData <- createDataPartition(y = finalData$Station, p = 0.8)
finalData<- finalData[indxData$Resample1,]
indxData <- createDataPartition(y = finalData$THour, p = 0.8)
finalData<- finalData[indxData$Resample1,]


# Create Normalized Variables based on the original Variables

finalData$start_date_n = as.numeric(finalData$start_time)
finalData$Cons_time_n = as.numeric(finalData$Cons_time)
finalData$WDay_n = as.numeric(finalData$WDay)
finalData$THour_n = as.numeric(finalData$THour)

# Data normalization
normalize <- function(x) {
  scale(x)
}


normalized_data <- as.data.frame(lapply(finalData[, 9:12], normalize))

finalData$start_date_n <- normalized_data$start_date_n
finalData$Cons_time_n <- normalized_data$Cons_time_n
finalData$WDay_n <- normalized_data$WDay_n
finalData$THour_n <- normalized_data$THour_n


# Using the interactions for clustering

finalData_2 =  orderedData
finalData_2$scene <- substring(finalData_2$scene, regexpr("sceneId=", finalData_2$scene) + 8, regexpr(">", finalData_2$scene) - 1)
finalData_2$dat=format(finalData_2$date, format="%Y:%m:%d")

finalData_for_interactions = finalData_2 %>% 
  group_by(Visitor=visitor, dat, Station=station, scene) %>% 
  summarise(count = 1) %>%
  spread(scene, count, fill=0)

rm(normalized_data, finalData_2, orderedData)
finalData_for_interactions <- finalData_for_interactions[1:2000,]


```

# Clustering Using the normalized Dataset
in this part we will apply K-means and Pam methods on th e normalized dataset, and we will choose the model with high average silhouette value as the final model.

## K-Means 
Starting with K-means clustering, we will try to cluster the observations into different numbers of clusters, and compare the averave silhouette values

Clusters Number range from 3 to 15, this range will be used for both methods k-means and pam.

```{r, warning=FALSE, message=FALSE}
clus_range <- seq(3, 10, 1)
k_meanslst <- list()
for (i in clus_range)
{
  k_meanslst[[i - 2]] <- kmeans(finalData[,c("start_date_n","Cons_time_n")], i)
}
k_meanslst <- k_meanslst [!sapply(k_meanslst, is.null)]

des <- daisy(as.matrix(finalData[,c("start_date_n","Cons_time_n")]))
kmeans_res <- sapply(k_meanslst, function(x) mean(silhouette(x$cl, des)[, 3]))
plot(clus_range, kmeans_res)


```


## Choosen K-Means Model
Here we selected the model with the highest average silhouette value, then we visualized the data based on the start date and the consumed time

```{r, warning=FALSE, message=FALSE}

k_means_mod <- k_meanslst [[match(max(kmeans_res),kmeans_res)]] 
finalData$k_m_clus = factor(k_means_mod$cluster) 

ggplot(finalData, aes(start_time, Cons_time)) +
  geom_point(aes(color=k_m_clus), size=2)+
  theme_bw()

```

We can see from this plot that the data is clustered into three different clusters which are well separated.

One of them mainly depends on the consumed time, and we think even that the data was normalized but there is a slight effect related to the values range, this cluster contains the visits which have consumed time bigger than three minutes.
while both of the second and the third clusters contain the records which have consumed time less than three (approximatly), but the visits in one of them started before jan 16, and the other after 16 Jan.   

## PAM
Here we will use Pam method to cluster the data, and also we will use different clusters numbers and compare the average silhouette values

```{r, warning=FALSE, message=FALSE}
pam_lst <- list()
for (i in clus_range)
{
  pam_lst[[i - 2]] <- pam(finalData[,c("start_date_n","Cons_time_n")], i)
}
pam_lst <- pam_lst [!sapply(pam_lst, is.null)]
pam_res <- sapply(pam_lst, function(x) mean(silhouette(x)[, 3]))
plot(clus_range, pam_res)

```

## Choosen Pam Model 

Also we will chose the model with the highest silhouette value.

```{r, warning=FALSE, message=FALSE}
pam_mod <- pam_lst [[match(max(pam_res),pam_res)]]
finalData$pam_clus = factor(pam_mod$clustering) 

ggplot(finalData, aes(start_time, Cons_time)) +
  geom_point(aes(color=pam_clus), size=2)+
  theme_bw()


```

We can see from the plot that it is simmilar to the k-means clusters, but this model is stable, and we will get the same clusters when we repeat it.

# Clustering Using Interactions Dataset
Here we have different approach to use for clustering, We will try to find the similar visits based on the visited pages in each visit. then we will cluster these visits.
we beleive that in real life this approach is practical especially for marketing purposes when you can recommend the rest of the visted pages for similar visitors.

we will apply K-means only after finding the best model

## K-means Clustering

```{r, warning=FALSE, message=FALSE}
clus_range <- seq(3, 20, 1)
col_names_intr <- colnames(finalData_for_interactions)
col_names_intr <- col_names_intr[4:33] 
k_meanslst_intr <- list()
for (i in clus_range)
{
  k_meanslst_intr[[i - 2]] <- kmeans(finalData_for_interactions[,col_names_intr], i)
}
k_meanslst_intr <- k_meanslst_intr [!sapply(k_meanslst_intr, is.null)]

des <- daisy(as.matrix(finalData_for_interactions[,col_names_intr]))
kmeans_res_intr <- sapply(k_meanslst_intr, function(x) mean(silhouette(x$cl, des)[, 3]))
plot(clus_range, kmeans_res_intr)



```

## Choosen Model based on Interactions Dataset and K-means method

```{r, warning=FALSE, message=FALSE}
print (paste("Clusters: ", match(max(kmeans_res_intr),kmeans_res_intr) + 2 , "silhouette : ", max(kmeans_res_intr)))

k_means_mod_Int <- k_meanslst_intr [[match(max(kmeans_res_intr),kmeans_res_intr)]]
finalData_for_interactions$k_m_i_clus = factor(k_means_mod_Int$cluster) 

for (cls in  unique(finalData_for_interactions$k_m_i_clus))
{
  temp <- filter(finalData_for_interactions, k_m_i_clus == cls)
  str <- paste("Cluster : ", cls, " Pages : ")
  for (col in col_names_intr)
  {
    su <- sum(temp[, col])
    if (su > 5 )
    {
      str <- paste(str, " ", col)  
    }
    
  }
  print (str)
}
```


# Statistics
here we will show some statistical information extracted from the these clusters

## K-means method using the normalized Dataset

### Visits in each Cluster

```{r, warning=FALSE, message=FALSE}
WeeksDays <- c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")

vis_data = finalData %>% 
  group_by(k_m_clus) %>%
  summarise(
    s_count = n()
  )     
x = vis_data$s_count
names(x) <- vis_data$k_m_clus
x <- sort(x, decreasing = TRUE)
barplot(x, las=2, xlab = "Clusters Visits", ylab = 'Count', ylim = c(0,2500)) 
```

### Visitors distribution in the Clusters

```{r, warning=FALSE, message=FALSE}

vis_data = finalData %>% 
  group_by(Visitor) %>%
  summarise(
    s_count = length(unique(k_m_clus))
  )

vis_data = vis_data %>% 
  group_by(s_count) %>%
  summarise(
    s_count_n = n()
  )

x = vis_data$s_count_n
names(x) <- vis_data$s_count
x <- sort(x, decreasing = TRUE)
barplot(x, las=2, xlab = "Visitor in each Clusters", ylab = 'Count', ylim = c(0,4000)) 
```


### Stations in each Cluster

```{r, warning=FALSE, message=FALSE}

vis_data = finalData %>% 
  group_by(k_m_clus, Station) %>%
  summarise(
    s_count = n()
  )     
x = vis_data$s_count
names(x) <- paste(vis_data$k_m_clus, "_", vis_data$Station)
x <- sort(x, decreasing = TRUE)
barplot(x, las=2, xlab = "Clusters Stations", ylab = 'Count', ylim = c(0,800)) 
```

We can see in this plot that the maximum visits count are in the second and third clusters in the station cnk05, while the minimum are in the first cluster in the stations cnk05, cnk02b, and cnk02a.

and we can find that all stations exist in all clusters.


### Start Hours in each Cluster

```{r, warning=FALSE, message=FALSE}
vis_data = finalData %>% 
  group_by(k_m_clus, THour) %>%
  summarise(
    s_count = n()
  )     

x = vis_data$s_count
names(x) <- paste(vis_data$k_m_clus, "_", vis_data$THour)
x <- sort(x, decreasing = TRUE)
barplot(x, las=2, xlab = "Clusters Hours", ylab = 'Count', ylim = c(0,300)) 
```


The main idea we can find in this plot is: the maximum visits count in each clusters are in the hours 12, 13, 14, which is the half of the day, whiel the minimum also for all of them at the begining and the end of the day.

### Days in each Cluster

```{r, warning=FALSE, message=FALSE}
vis_data = finalData %>% 
  group_by(k_m_clus, WDay) %>%
  summarise(
    s_count = n()
  )     

x = vis_data$s_count
names(x) <- paste(vis_data$k_m_clus, "_", WeeksDays[vis_data$WDay + 1])
x <- sort(x, decreasing = TRUE)
barplot(x, las=2, xlab = "Clusters Days", ylab = 'Count', ylim = c(0,500)) 
```

here is the distribution of the visits count in each cluster for each day, the minimum in the first cluster

## K-means method using the Interactions Dataset

### Stations in each Cluster

```{r, warning=FALSE, message=FALSE}
vis_data = finalData_for_interactions %>% 
  group_by(k_m_i_clus, Station) %>%
  summarise(
    s_count = n()
  )     
x = vis_data$s_count
names(x) <- paste(vis_data$k_m_i_clus, "_", vis_data$Station)
x <- sort(x, decreasing = TRUE)
barplot(x, las=2, xlab = "Clusters Stations", ylab = 'Count', ylim = c(0,450)) 
```

We can see that most of the stations are in the first cluster, while other clusters contain few different stations 

### Days in each Cluster

```{r, warning=FALSE, message=FALSE}
vis_data = finalData_for_interactions %>% 
  group_by(k_m_i_clus, dat) %>%
  summarise(
    s_count = n()
  )     


x = vis_data$s_count
#names(x) <- paste(vis_data$k_m_i_clus, "_", WeeksDays[as.POSIXlt(as.Date(vis_data$dat, "%Y:%m:%d"))$wday])
x <- sort(x, decreasing = TRUE)
barplot(x, las=2, xlab = "Clusters Days", ylab = 'Count', ylim = c(0,450)) 

```

we can see that some clusters contain alot of visits in the same day.
in this plot the cluster 1 includes alot of vists in the Tuesday, Wednesday, then Monday


# Hierarchical clustering - top down (divisive) - from large clusters to small ones - NEVEN

First, we will use hierarchical clustering - top down (divisive) - from large clusters to small ones. 
Features we use are:
  
  * start hour
  * end hour
  * weekday
  * time of consumption
  * station
    
We decomposed clustering and first considered pairs of features for visitor clustering. In the end we used all features
for final clustering.

```{r, warning=FALSE, message=FALSE}
finalData_h <- orderedDataFinal[1:1000,]
finalData_h <- na.omit(finalData_h)
rm(orderedDataFinal, finalData, finalData_for_interactions)
finalData_h$start_hour <- as.numeric(as.POSIXlt(finalData_h$start_time)$hour)
finalData_h$end_hour <- as.numeric(as.POSIXlt(finalData_h$send_time)$hour)
finalData_h$month <- as.numeric(as.POSIXlt(finalData_h$start_time)$mon)
finalData_h$Cons_time <- as.numeric(finalData_h$Cons_time)
finalData_h$Station <- as.character(finalData_h$Station)


```

## Euclidian Distances For Scaled Dataset

```{r, warning=FALSE, message=FALSE}
start_hour__end_hour <- scale(dist(finalData_h[,c("start_hour", "end_hour")]))

start_hour__wday <- scale(dist(finalData_h[,c("start_hour", "WDay")]))

start_hour__cons_time <- scale(dist(finalData_h[,c("start_hour", "Cons_time")]))

start_hour__station <- scale(dist(finalData_h[,c("start_hour", "Station")]))

wday__cons_time <- scale(dist(finalData_h[,c("WDay", "Cons_time")]))

wday__station <- scale(dist(finalData_h[,c("WDay", "Station")]))

cons_time__station <- scale(dist(finalData_h[,c("Cons_time", "Station")]))

all <- scale(dist(finalData_h[,c("start_hour", "end_hour", "Cons_time", "Station", "WDay")]))


```

## Hierarchical clustering

### Start Hour - End Hour
```{r, warning=FALSE, message=FALSE}
library(ape)
library(RColorBrewer)
cols <- brewer.pal(3,"Set1")

# start hour - end hour
hc <- agnes(start_hour__end_hour, method="ward")
finalData_h$labels = factor(cutree(hc, k=4))
ggplot(finalData_h, aes(start_hour, end_hour, label=Visitor, color=labels)) +
  geom_text(size=3) + 
  theme_bw()
hc <- as.phylo(as.hclust(agnes(start_hour__end_hour, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])

rm(start_hour__end_hour)
```

Visitors can be grouped by the hour they started using stations and the hour they ended using stations. It can be seen
that we can segment visitors in four different clusters according to those two features. It can be concluded that 
visitors tend to start and end using stations in the same hour. 

### Start Hour - Weekday
```{r, warning=FALSE, message=FALSE}

hc <- agnes(start_hour__wday, method="ward")
finalData_h$labels = factor(cutree(hc, k=4))
ggplot(finalData_h, aes(start_hour, WDay, label=Visitor, color=labels)) +
  geom_text(size=3) + 
  theme_bw()
hc <- as.phylo(as.hclust(agnes(start_hour__wday, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])
rm(start_hour__wday)
```

Visitors are grouped by the hour they started using stations and the weekday they started using stations. It can
be seen that we can again segment visitors in four different clusters according to those two features. Clusters are
more-less of the same size, so it can be concluded that visitors have almost the same habits during whole week.

### Start Hour - Time of Consuming
```{r, warning=FALSE, message=FALSE}
# start hour - the time of consuming
hc <- agnes(start_hour__cons_time, method="ward")
finalData_h$labels = factor(cutree(hc, k=2))
ggplot(finalData_h, aes(start_hour, Cons_time, label=Visitor, color=labels)) +
  geom_text(size=3) + 
  theme_bw()
hc <- as.phylo(as.hclust(agnes(start_hour__cons_time, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])
rm(start_hour__cons_time)
```
Visitors are grouped by the hour they started using stations and the time of consuming they started using stations. It
can be seen that we can segment visitors in two different clusters according to those two features. First cluster is 
much more bigger than the second one. There are less visitors who tend to consume stations very long at given hour.



### Start Hour - Station
```{r, warning=FALSE, message=FALSE}

hc <- agnes(start_hour__station, method="ward")
finalData_h$labels = factor(cutree(hc, k=4))
ggplot(finalData_h, aes(start_hour, Station, label=Visitor, color=labels)) +
  geom_text(size=3) + 
  theme_bw()
hc <- as.phylo(as.hclust(agnes(start_hour__station, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])

rm(start_hour__station)
```

Visitors are grouped by the hour they started using stations and the stations they use. It can be seen that we can 
segment visitors in four different clusters according to those two features. Visitors tend to use different machines
in four different time periods.



### Weekday - Time of Consuming

```{r, warning=FALSE, message=FALSE}
hc <- agnes(wday__cons_time, method="ward")
finalData_h$labels = factor(cutree(hc, k=2))
ggplot(finalData_h, aes(WDay, Cons_time, label=Visitor, color=labels)) +
  geom_text(size=3) + 
  theme_bw()
hc <- as.phylo(as.hclust(agnes(wday__cons_time, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])

rm(wday__cons_time)

```

Visitors are grouped by the weekday they started using stations and the time of consuming. It can be seen that we can 
segment visitors in two different clusters according to those two features. Consuming time is very similar for each
weekday and those visitors are in the first, larger cluster and in second, smaller cluster are outliers.


### Weekday - Station
```{r, warning=FALSE, message=FALSE}
# weekday - station
hc <- agnes(wday__station, method="ward")
finalData_h$labels = factor(cutree(hc, k=3))
ggplot(finalData_h, aes(WDay, Station, label=Visitor, color=labels)) +
  geom_text(size=3) + 
  theme_bw()
hc <- as.phylo(as.hclust(agnes(wday__station, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])

rm(wday__station)
```
Visitors are grouped by the weekday they started using stations and the station. It can be seen that we can 
segment visitors in three different clusters according to those two features. Each of the stations have almost equal
number of visitors each weekday.


### Time of Consuming - Station
```{r, warning=FALSE, message=FALSE}
# time of consuming - station
hc <- agnes(cons_time__station, method="ward")
finalData_h$labels = factor(cutree(hc, k=3))
ggplot(finalData_h, aes(Cons_time, Station, label=Visitor, color=labels)) +
  geom_text(size=3) + 
  theme_bw()
hc <- as.phylo(as.hclust(agnes(cons_time__station, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])

rm(cons_time__station)
```
Visitors are grouped by the time of consuming and the station. It can be seen that we can segment visitors in three 
different clusters according to those two features. The largest cluster is cluster with visitors that use stations 
less than cca. 20 seconds and two smaller clusters are clusters with visitors that use stations between 20 and 90 
seconds and more than 90 seconds.

### All Features

```{r, warning=FALSE, message=FALSE}
# all features
hc <- agnes(all, method="ward")
finalData_h$labels = factor(cutree(hc, k=4))
hc <- as.phylo(as.hclust(agnes(all, method="ward")))
par(mar=c(1,1,2,1), xpd=NA)
plot(as.phylo(hc), type = "unrooted", cex = 0.8,
     tip.color = cols[finalData_h$labels])
rm(all)
```

Visitors are grouped using all features. It can be seen that if we consider all features, we can group visitors in 
four different groups, there are four main types of visitors, but also it is possible to divide them even into 
smaller clusters, as it can be seen from the graph.

# Conclusion

How to define the similarity measure between visitors? As above, we calculated similarity between visitors using different models:

- start_hour, end_hour, weekday, consumed_time and station features with Hierarchical clustering.
- Start Time and Consumed Time with k-means and pam
- visited pages with k-means 

Is the population homogenous or heterogonous. If heterogeneous then how many groups you can derive/define? Population
is heterogeneous as it can be seen from the last graph where we used all features. Four main clusters can be derived,
but also we can divide those four classter into smaller ones.


How to characterize different groups of visitors? In different groups of visitors, visitors started using stations
on different weekdays, time of consumption is different, hours when they started using station are different, 
machines they are using are different and so on.

And For K-means and Pam We noticed that:

One Cluster depends on the consumed time, and this cluster contains the visits which have consumed time bigger than three minutes.
while both of the second and the third clusters contain the records which have consumed time less than three (approximatly), but the visits in one of them started before jan 16, and the other after 16 Jan. 

And the visted pages table describe the clusters when we used the interactions dataset.

Is there a pattern in stations that visitor tends to visit? We analyzed only few stations because of computational 
speed and visitors tend to visit those stations equally.
And we have some plots with description to show some statistical information about the data.



