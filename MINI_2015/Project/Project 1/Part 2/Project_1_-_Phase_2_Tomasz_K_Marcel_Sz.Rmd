---
title: "Project 1 - Phase 2"
author: "Tomasz K, Marcel Sz"
date: "2015-10-14"
output: 
  html_document:
    toc: TRUE
---

# Introduction

The solution presents performance of few different classifiers:

- K-nearest neighbors
- SVM
- Decision Tree
- Random Forest
- Naive Bayes

Classifiers are evaluated using:

- Accuracy and Kappa using confusion Matrix
- K-fold cross validation
- Bootstrap
- ROC Curves
- Area Under Curve


# Preparing data

The 'australian' dataset comprising credit card data is loaded from: http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat

The dataset is split into training and testing set (with 75-25 ratio).

```{r, warning=FALSE, message=FALSE}
library(caret)
library(ROCR)
library(randomForest)
library(e1071)
library(party)
library(rpart)
library(Hmisc)


australian = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat",
        sep=" ",header=F,fill=FALSE,strip.white=T)

originalaustralian <-australian
australian$V15 <- factor(australian$V15)

# Training/Testing set partition 
indxTrain <- createDataPartition(y = australian$V15, p = 0.75)
australianTrain<- australian[indxTrain$Resample1,]
australianTest <- australian[-indxTrain$Resample1,]
australianTrain$V15 <- factor(australianTrain$V15)
```

# Random Forest

```{r, warning=FALSE, message=FALSE}
fullForest <- randomForest(V15 ~ ., data = australian, importance = TRUE, na.action = na.omit)
```

We will use random forest to extract, say 5, most important features.

```{r, warning=FALSE, message=FALSE}
importance(fullForest)
varImpPlot(fullForest)
```

According to both accuracy and gini feature importance indicators the important variables are: V8, V14, V5, V10, V7.
Worst features according to the plot are: V1, V11, V2, V12, V6

# K - Nearest Neighbors 

## Data Normalization

Because K-nn classifier uses distance calculation to determine nearest neighbors we have to normalize feature values.

```{r, warning=FALSE, message=FALSE}
australianNormalized <- australian 
australianNormalized$V15 <- factor(australianNormalized$V15)
australianNormalized$V1 <- scale(australianNormalized$V1)
australianNormalized$V2 <- scale(australianNormalized$V2)
australianNormalized$V3 <- scale(australianNormalized$V3)
australianNormalized$V4 <- scale(australianNormalized$V4)
australianNormalized$V5 <- scale(australianNormalized$V5)
australianNormalized$V6 <- scale(australianNormalized$V6)
australianNormalized$V7 <- scale(australianNormalized$V7)
australianNormalized$V8 <- scale(australianNormalized$V8)
australianNormalized$V9 <- scale(australianNormalized$V9)
australianNormalized$V10 <- scale(australianNormalized$V10)
australianNormalized$V11 <- scale(australianNormalized$V11)
australianNormalized$V12 <- scale(australianNormalized$V12)
australianNormalized$V13 <- scale(australianNormalized$V13)
australianNormalized$V14 <- scale(australianNormalized$V14)

# Training/Testing set partition 
indxTrain <- createDataPartition(y = australianNormalized$V15, p = 0.75)
australianNormalizedTrain<- australianNormalized[indxTrain$Resample1,]
australianNormalizedTest <- australianNormalized[-indxTrain$Resample1,]
```

## K-fold Cross Validation

To obtain best accuracy we must try several values of k, say 40, and find best one.

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)
model <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australianNormalizedTrain, trControl=train_control, method="knn", tuneLength = 40)
predTab <- predict(model, australianNormalizedTest)
confusion_matrix <- confusionMatrix(predTab, australianNormalizedTest$V15)
confusion_matrix
cvk = model$bestTune[[1]]
```

## Bootstrap

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="boot", number=10)
model <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australianNormalizedTrain, trControl=train_control, method="knn", tuneLength = 40)
predTab <- predict(model, australianNormalizedTest)
confusion_matrix <- confusionMatrix(predTab, australianNormalizedTest$V15)
confusion_matrix
bk = model$bestTune[[1]]
```

#SVM

## Training and testing sets

```{r, warning=FALSE, message=FALSE}
svm1 <- svm(V15 ~ V8 + V14 + V5 + V10 + V7, data = australianTrain, kernel='linear', cost=10, scale=FALSE, probability = TRUE)
predictions <- predict(svm1, australianTest)
confusion_matrix <- confusionMatrix(predictions, australianTest$V15)
confusion_matrix
```

## K-fold cross validation

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)
svm2 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="svmLinear")
predictions <- predict(svm2, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix
```

## Bootstrap

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="boot", number=10)
svm3 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="svmLinear")
predictions <- predict(svm3, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix
```

#Random forest

## Training and testing sets

```{r, warning=FALSE, message=FALSE}
forest1 <- randomForest(V15 ~ V8 + V14 + V5 + V10 + V7, data = australianTrain, importance = TRUE, na.action = na.omit)
predictions <- predict(forest1, australianTest)
confusion_matrix <- confusionMatrix(predictions, australianTest$V15)
confusion_matrix
```

## K-fold cross validation

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)
forest2 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="rf")
predictions <- predict(forest2, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix
```

## Bootstrap

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="boot", number=10)
forest3 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="rf")
predictions <- predict(forest3, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix
```

#Decision Tree

## Training and testing sets
```{r, warning=FALSE, message=FALSE}
# ctree
ctree1 <- ctree(V15 ~ V8 + V14 + V5 + V10 + V7, data=australianTrain)
predictions <- predict(ctree1, australianTest)
confusion_matrix <- confusionMatrix(predictions, australianTest$V15)
confusion_matrix
plot(ctree1)

# rtree
rtree1 <- rpart(V15 ~ V8 + V14 + V5 + V10 + V7, data=australianTrain, method="class")
predictions <- predict(rtree1, australianTest)
plot(rtree1)
text(rtree1)
```

## K-fold cross validation

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)

# ctree
ctree2 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="ctree")
predictions <- predict(ctree2, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix

# rtree
rtree2 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="rpart")
predictions <- predict(rtree2, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix
```

## Bootstrap

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="boot", number=10)

# ctree
ctree2 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="ctree")
predictions <- predict(ctree2, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix

# rtree
rtree2 <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=australian, trControl=train_control, method="rpart")
predictions <- predict(rtree2, australian)
confusion_matrix <- confusionMatrix(predictions, australian$V15)
confusion_matrix
```

# Naive Bayes

## Data Preparation
For naive bayes classifier it is good to split the feature values into ranges of equal size (with regard to number of observations).

Spliting is achieved using cut2 function, factor will be used when the value either one or zero.

```{r, warning=FALSE, message=FALSE}
aust_equal_size <- originalaustralian
for (i in c(2, 3, 4, 5, 6, 7, 10, 12, 13, 14)) {
  aust_equal_size[,i]<- cut2(australian[,i], g=3)
}
aust_equal_size$V15 <- factor(ifelse(aust_equal_size$V15 == 0, "Yes", "No")) 
aust_equal_size$V1 <-factor(aust_equal_size$V1)
aust_equal_size$V8 <-factor(aust_equal_size$V8)
aust_equal_size$V9 <-factor(aust_equal_size$V9)
aust_equal_size$V11 <-factor(aust_equal_size$V11)
summary(aust_equal_size)

# Training/Testing set partition 
indxTrain <- createDataPartition(y = aust_equal_size$V15, p = 0.8)
aust_equal_sizeTrain<- aust_equal_size[indxTrain$Resample1,]
aust_equal_sizeTest <- aust_equal_size[-indxTrain$Resample1,]
```

## K-fold cross validation

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)
nb_k_model <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=aust_equal_sizeTrain, trControl=train_control, method="nb")
nb_k_predTab <- predict(nb_k_model, aust_equal_sizeTest)
confusion_matrix <- confusionMatrix(nb_k_predTab, aust_equal_sizeTest$V15)
confusion_matrix
```


## Bootstraps

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="boot", number=10)
nb_b_model <- train(V15 ~ V8 + V14 + V5 + V10 + V7, data=aust_equal_sizeTrain, trControl=train_control, method="nb")
nb_b_predTab <- predict(nb_b_model, aust_equal_sizeTest)
confusion_matrix <- confusionMatrix(nb_b_predTab, aust_equal_sizeTest$V15)
confusion_matrix
```

# ROC (Receiver Operating Characteristics)

```{r, warning=FALSE, message=FALSE}
# SVM
svm.prob <- attributes(predict(svm1, newdata=australianTest, probability = TRUE))$probabilities[, 2]
svm.fit.pred =  prediction(svm.prob, australianTest$V15)
svm.fit.perf = performance(svm.fit.pred, "tpr", "fpr")
plot(svm.fit.perf, col="blue")

# Naive Bayes
nb_k.prob <- predict(nb_k_model, aust_equal_sizeTest, type="prob")
nb_k.fit.pred <- prediction(nb_k.prob[,2], aust_equal_sizeTest$V15)
nb_k.fit.perf <- performance(nb_k.fit.pred, "tpr", "fpr")
nb_k.fit_auc = performance(nb_k.fit.pred,"auc")
plot(nb_k.fit.perf, col="pink", add=TRUE)

nb_b.prob <- predict(nb_b_model, aust_equal_sizeTest, type="prob")
nb_b.fit.pred <- prediction(nb_b.prob[,2], aust_equal_sizeTest$V15)
nb_b.fit.perf <- performance(nb_b.fit.pred, "tpr", "fpr")
nb_b.fit_auc = performance(nb_b.fit.pred,"auc")
plot(nb_b.fit.perf, col="purple", add=TRUE)

# K-nearest Neighbors 
for(k in c(cvk, bk))
{
  knn <- knn3(V15 ~ V8 + V14 + V5 + V10 + V7, data=australianNormalizedTrain, k = k )
  kntab <- predict(knn, newdata = australianNormalizedTest, type="prob")[, 2]
  fit.pred <- prediction(kntab, australianNormalizedTest$V15)
  fit.pref <- performance(fit.pred, "tpr", "fpr")
  fit.perf_auc = performance(fit.pred,"auc")
  if (k == cvk)
  {
    kfknn_auc<-fit.perf_auc@y.values[[1]]
    plot(fit.pref, col="brown", add=TRUE)
  }
  else
  {
    bknn_auc<-fit.perf_auc@y.values[[1]]
    plot(fit.pref, col="orange", add = TRUE)
  }
}

# Random Forest
rf.prob <- predict(forest1, australianTest, type="prob")
rf.fit.pred = prediction(rf.prob[,2], australianTest$V15)
rf.fit.perf = performance(rf.fit.pred,"tpr","fpr")
plot(rf.fit.perf, col="red", add=TRUE)

# Decision Tree
ctree.prob <- do.call(rbind, predict(ctree1, newdata=australianTest, type="prob"))
ctree.fit.pred <- prediction(ctree.prob[,2], australianTest$V15)
ctree.fit.perf = performance(ctree.fit.pred,"tpr","fpr")
plot(ctree.fit.perf, col="gold", add=TRUE)

rtree.prob <- predict(rtree1, newdata=australianTest, type="prob")
rtree.fit.pred <- prediction(rtree.prob[,2], australianTest$V15)
rtree.fit.perf = performance(rtree.fit.pred,"tpr","fpr")
plot(rtree.fit.perf, col="yellow", add=TRUE)

abline(a=0, b=1)
```

### Legend:
- K-nearest neighbors - brown, orange
- SVM - blue
- Random Forest - red
- Decision tree - gold, yellow
- Naive Bayes - pink, purple

# AUC (Area Under the Curve)
The area under the curve is computed for all classifers

## SVM

```{r, warning=FALSE, message=FALSE}
svm.fit.perf = performance(svm.fit.pred,"auc")
svm.fit.perf@y.values[[1]]
```

## Random forest

```{r, warning=FALSE, message=FALSE}
rf.fit.perf = performance(rf.fit.pred,"auc")
rf.fit.perf@y.values[[1]]
```

## Decision tree

```{r, warning=FALSE, message=FALSE}
ctree.fit.perf = performance(ctree.fit.pred,"auc")
ctree.fit.perf@y.values[[1]]
rtree.fit.perf = performance(rtree.fit.pred,"auc")
rtree.fit.perf@y.values[[1]]
```

## K-nearest Neighbors

```{r, warning=FALSE, message=FALSE}
kfknn_auc
bknn_auc
```

## Naive Bayes

```{r, warning=FALSE, message=FALSE}
nb_b.fit_auc@y.values[[1]]
nb_k.fit_auc@y.values[[1]]
```

# Conclusions
Most classifiers (apart from random forest) oscillate around 0.85-0.87 accuracy, with Naive Bayes scoring a little worse (0.81) because of the need of categorisation of feature values. But, the Random Forest proves to be absolute leader with 0.95 accuracy and 0.91 kappa when used in combination with k-folds cross validation / bootstrap. 
When analyzing area under curve, the best score is achieved by SVM - 0.94 and by K-nearest neighbors - 0.93, with also quite good score of random forest - 0.91.
