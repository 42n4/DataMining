---
title: "Project #1"
author: "Margareta Kusan, Neven Piculjan"
date: "November 5, 2015"
output: 
  html_document:
    toc : TRUE
---
# Introduction

In the second phase we should try any number of classifier we wish in order to create the best possible classifier.

# Dataset loading and preprocessing

```{r}
australian = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat",
                               sep=" ",header=F,col.names=c("A1", "A2", "A3", "A4", 
                                                            "A5","A6", "A7", "A8", "A9","A10",
                                                            "A11", "A12", "A13","A14", "A15"),
                               fill=FALSE,strip.white=T)
head(australian)
```

Casting australian$A15 to factor variable
```{r}
australian$A15 <- factor(australian$A15)
```

Summary
```{r}
summary(australian)
```

# Dividing dataset into training and testing set

Training set: 75 %
Testing set: 25 %
 
```{r}
library(caret)

set.seed(1313)
indxTrain <- createDataPartition(y = australian$A15, p = 0.75)
str(indxTrain)

australianTrain <- australian[indxTrain$Resample1,]
australianTest <- australian[-indxTrain$Resample1,]
```

# k-NN classifier

Finding best k

```{r}
tuneK <- 1:200
performance <- sapply(tuneK, function(k) {
  knnFit <- knn3(A15 ~ ., data = australianTrain, k=k)
  tab <- table(true = australianTest$A15,
               predict = predict(knnFit, australianTest, type="class"))
  sum(diag(tab)) / sum(tab)
}) 

df <- data.frame(tuneK, performance)

ggplot(df, aes(tuneK, performance)) +
  geom_point() + 
  geom_smooth(se=FALSE, span=0.1, size=2) +
  theme_bw()
```

Training and testing k-nn

```{r}
knnFit <- knn3(A15 ~ ., data = australianTrain, k=23) 
pred <- predict(knnFit, australianTest, type = "class")

length(australianTest$A15)
length(pred)

tab <- table(true = australianTest$A15, predicted = pred)

knn_accuracy = sum(diag(tab)) / sum(tab)
```

Feature importance

```{r}
train_control <- trainControl(method="cv", number=10)

model <- train(A15~., data=australian, trControl=train_control, method="knn")
varImp(model, scale=TRUE)
plot(varImp(model))
```

Finding best k for k-nn with 3 most important features

```{r}
tuneK <- 1:200
performance <- sapply(tuneK, function(k) {
  knnFit <- knn3(A15 ~ A8+A10+A9, data = australianTrain, k=k)
  tab <- table(true = australianTest$A15,
               predict = predict(knnFit, australianTest, type="class"))
  sum(diag(tab)) / sum(tab)
}) 

df <- data.frame(tuneK, performance)

ggplot(df, aes(tuneK, performance)) +
  geom_point() + 
  geom_smooth(se=FALSE, span=0.1, size=2) +
  theme_bw()
```

Training and testing k-nn with 3 most important features

```{r}
knnFit <- knn3(A15 ~ A8+A10+A9, data = australianTrain, k=23) 
pred <- predict(knnFit, australianTest, type = "class")

length(australianTest$A15)
length(pred)

tab <- table(true = australianTest$A15, predicted = pred)

knn_three_important_accuracy = sum(diag(tab)) / sum(tab)
```

# Random Forest classifier

Training and testing Random Forest

```{r}
library(randomForest)
ffit <- randomForest(A15 ~ .,   data=australian, importance = TRUE, mtry=3)
pred <- predict(ffit, type="class")

tab <- table(true = australian$A15, predicted = pred)
random_forest_accuracy = sum(diag(tab)) / sum(tab)
```

Feature importance

```{r}
train_control <- trainControl(method="cv", number=10)

model <- train(A15~., data=australian, trControl=train_control, method="cforest")
varImp(model, scale=TRUE)
plot(varImp(model))
```

Training and testing Random Forest with 3 most important features

```{r}
ffit <- randomForest(A15~A8+A9+A5,   data=australian, importance = TRUE, mtry=3)
pred <- predict(ffit, type="class")

tab <- table(true = australian$A15, predicted = pred)
random_forest_three_important_accuracy = sum(diag(tab)) / sum(tab)
```

# Naive Bayes classifier

Training and testing with k-fold cross validation

```{r}

# without categorizing each variable
library(caret)
library(e1071)

folds <- createFolds(australian$A15, k = 10)

perf <- sapply(folds, function(fold) {
  nbc <- naiveBayes(A15 ~ ., data=australian[-fold,])
  pred <- predict(nbc, australian[fold,])
  mean(australian$A15[fold] == pred)
})

barplot(perf, horiz = TRUE, las=1)

naive_bayes_without_categorizing_accuracy = mean(perf)

# with categorizing each variable
australianb <- australian

for (i in 1:14) {
  australianb[,i] <- cut(australianb[,i], 3)
}

library(e1071)
folds <- createFolds(australianb$A15, k = 10)

perf <- sapply(folds, function(fold) {
  nbc <- naiveBayes(A15 ~ ., data=australianb[-fold,])
  pred <- predict(nbc, australianb[fold,])
  mean(australianb$A15[fold] == pred)
})

barplot(perf, horiz = TRUE, las=1)

naive_bayes_with_categorizing_accuracy = mean(perf)
```

Feature importance

```{r}
train_control <- trainControl(method="cv", number=10)

# without categorizing each variable
model <- train(A15~., data=australian, trControl=train_control, method="nb")
varImp(model, scale=TRUE)
plot(varImp(model))

# with categorizing each variable
model <- train(A15~., data=australianb, trControl=train_control, method="nb")
varImp(model, scale=TRUE)
plot(varImp(model))
```

Training and testing with k-fold cross validation, but only 3 most important features

```{r}
library(e1071)

# without categorizing each variable
folds <- createFolds(australian$A15, k = 10)

perf <- sapply(folds, function(fold) {
  nbc <- naiveBayes(A15 ~ A8+A10+A9, data=australian[-fold,])
  pred <- predict(nbc, australian[fold,])
  mean(australian$A15[fold] == pred)
})

barplot(perf, horiz = TRUE, las=1)

naive_bayes_without_categorizing_three_important_accuracy = mean(perf)

# with categorizing each variable
folds <- createFolds(australianb$A15, k = 10)

perf <- sapply(folds, function(fold) {
  nbc <- naiveBayes(A15 ~ A8+A10+A9, data=australianb[-fold,])
  pred <- predict(nbc, australianb[fold,])
  mean(australianb$A15[fold] == pred)
})

barplot(perf, horiz = TRUE, las=1)

naive_bayes_with_categorizing_three_important_accuracy = mean(perf)
```

# ROC curve and AUC for all three classifiers

```{r}
library(randomForest)
library(ROCR)

ffit <- randomForest(A15 ~ .,   data=australian, importance = TRUE, mtry=3)
prob <- predict(ffit, type="prob")[,2]

ffit_three_important <- randomForest(A15~A8+A9+A5,   data=australian, importance = TRUE, mtry=3)
prob_three_important <- predict(ffit_three_important, type="prob")[,2]

ffit2 <- knn3(A15 ~ .,   data=australian, k=23)
prob2 <- predict(ffit2, newdata = australian, type="prob")[,2]

ffit2_three_important <- knn3(A15 ~ A8+A10+A9,   data=australian, k=23)
prob2_three_important <- predict(ffit2_three_important, newdata = australian, type="prob")[,2]

ffit3 <- naiveBayes(A15~.,   data=australian)
prob3 <- predict(ffit3, newdata = australian, type="raw")[,2]

ffit3b <- naiveBayes(A15~.,   data=australianb)
prob3b <- predict(ffit3b, newdata = australianb, type="raw")[,2]

ffit3_three_important <- naiveBayes(A15~A8+A10+A9,   data=australian)
prob3_three_important <- predict(ffit3_three_important, newdata = australian, type="raw")[,2]

ffit3b_three_important <- naiveBayes(A15~A8+A10+A9,   data=australianb)
prob3b_three_important <- predict(ffit3b_three_important, newdata = australian, type="raw")[,2]



fit.pred = prediction(prob, australian$A15)
fit.perf = performance(fit.pred,"tpr","fpr")
plot(fit.perf, col="red")

fit.pred_three_important = prediction(prob_three_important, australian$A15)
fit.perf_three_important = performance(fit.pred_three_important,"tpr","fpr")
plot(fit.perf_three_important, col="green", add=TRUE)

fit.pred2 = prediction(prob2, australian$A15)
fit.perf2 = performance(fit.pred2,"tpr","fpr")
plot(fit.perf2, col="blue", add=TRUE)

fit.pred2_three_important = prediction(prob2_three_important, australian$A15)
fit.perf2_three_important = performance(fit.pred2_three_important,"tpr","fpr")
plot(fit.perf2_three_important, col="yellow", add=TRUE)

fit.pred3 = prediction(prob3, australian$A15)
fit.perf3 = performance(fit.pred3,"tpr","fpr")
plot(fit.perf3, col="black", add=TRUE)

fit.pred3b = prediction(prob3b, australianb$A15)
fit.perf3b = performance(fit.pred3b,"tpr","fpr")
plot(fit.perf3b, col="purple", add=TRUE)

fit.pred3_three_important = prediction(prob3_three_important, australian$A15)
fit.perf3_three_important = performance(fit.pred3_three_important,"tpr","fpr")
plot(fit.perf3_three_important, col="orange", add=TRUE)

fit.pred3b_three_important = prediction(prob3b_three_important, australianb$A15)
fit.perf3b_three_important = performance(fit.pred3b_three_important,"tpr","fpr")
plot(fit.perf3b_three_important, col="brown", add=TRUE)

abline(a=0,b=1)



fit.pred = prediction(prob, australian$A15)
fit.perf = performance(fit.pred,"auc")
fit.perf@y.values[[1]]

fit.pred_three_important = prediction(prob_three_important, australian$A15)
fit.perf_three_important = performance(fit.pred_three_important,"auc")
fit.perf_three_important@y.values[[1]]

fit.pred2 = prediction(prob2, australian$A15)
fit.perf2 = performance(fit.pred2,"auc")
fit.perf2@y.values[[1]]

fit.pred2_three_important = prediction(prob2_three_important, australian$A15)
fit.perf2_three_important = performance(fit.pred2_three_important,"auc")
fit.perf2_three_important@y.values[[1]]

fit.pred3 = prediction(prob3, australian$A15)
fit.perf3 = performance(fit.pred3,"auc")
fit.perf3@y.values[[1]]

fit.pred3b = prediction(prob3b, australian$A15)
fit.perf3b = performance(fit.pred3b,"auc")
fit.perf3b@y.values[[1]]

fit.pred3_three_important = prediction(prob3_three_important, australian$A15)
fit.perf3_three_important = performance(fit.pred3_three_important,"auc")
fit.perf3_three_important@y.values[[1]]

fit.pred3b_three_important = prediction(prob3b_three_important, australianb$A15)
fit.perf3b_three_important = performance(fit.pred3b_three_important,"auc")
fit.perf3b_three_important@y.values[[1]]
```

# Results

```{r}
accuracy <- matrix (c(
random_forest_accuracy,
random_forest_three_important_accuracy,
knn_accuracy,
knn_three_important_accuracy,
naive_bayes_without_categorizing_accuracy,
naive_bayes_with_categorizing_accuracy,
naive_bayes_without_categorizing_three_important_accuracy,
naive_bayes_with_categorizing_three_important_accuracy
), byrow=FALSE)
rownames(accuracy) <- c("RF","RF - 3 imp.","knn", "knn - 3 imp.", "NB", "NBb", "NB - 3 imp.", "NBb - 3 imp")
colnames(accuracy) <- c("accuracy")
accuracy <- as.table(accuracy)


AUC <- matrix (c(
fit.perf@y.values[[1]],
fit.perf_three_important@y.values[[1]],
fit.perf2@y.values[[1]],
fit.perf2_three_important@y.values[[1]],
fit.perf3@y.values[[1]],
fit.perf3b@y.values[[1]],
fit.perf3_three_important@y.values[[1]],
fit.perf3b_three_important@y.values[[1]]
), byrow=FALSE)
rownames(AUC) <- c("RF","RF - 3 imp.","knn", "knn - 3 imp.", "NB", "NBb", "NB - 3 imp.", "NBb - 3 imp")
colnames(AUC) <- c("AUC")
AUC <- as.table(AUC)
```

