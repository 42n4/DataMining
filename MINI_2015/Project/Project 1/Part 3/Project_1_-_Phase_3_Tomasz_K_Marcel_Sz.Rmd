---
title: "Project 1 - Phase 3"
author: "Tomasz K, Marcel Sz"
date: "2015-11-12"
output: 
  html_document:
    toc: TRUE
---

# Introduction

This report will summarize number of classifiers pointing out performance of each. It will present conclusive recommendations regarding best/worst classifiers and best/worst features.

# Preparing data

As in the previous phases the 'australian' dataset is used.

```{r, warning=FALSE, message=FALSE}
library(caret)
library(ROCR)

australian = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat",
        sep=" ",header=F,fill=FALSE,strip.white=T)

australian$V15 <- factor(ifelse(australian$V15 == 0, "No", "Yes"))
```

Before using the variables to train model of each classifier we will extract most/least important features using random forest classifier.

```{r, warning=FALSE, message=FALSE}
train_control <- trainControl(method="cv", number=10)
rf <- train(V15 ~ ., data=australian, trControl=train_control, method="rf")
importance <- varImp(rf, scale=FALSE)
print(importance)
plot(importance)
```

The best features are: V8, V7, V10, V14.

The worst features are: V1, V11, V12, V4.

Now, using extracted best features we can train proposed classifiers and see their performance.

# Performance of classifiers

We will check the performance of following classifiers using caret's 'train' function:

- K-Nearest Neighbors
- SVM with Linear Kernel
- Random Forest
- Conditional Inference Tree
- LDA
- QDA
- Naive Bayes
- SVM with Radial Basis Function Kernel
- Boosted Logistic Regression
- eXtreme Gradient Boosting
- Bagged CART
- Partial Least Squares

First we will train each classifier using all variables.
```{r, warning=FALSE, message=FALSE}
classifiers <- c('knn', 'svmLinear', 'rf', 'ctree', 'lda', 'qda', 'nb', 'svmRadial', 'LogitBoost', 'xgbTree', 'treebag', 'pls')
info <- sapply(classifiers, function (met) {
  train_control <- trainControl(method="cv", number=10)
  model <- train(V15 ~ ., method=met, data=australian, trControl=train_control, preProcess=c('scale', 'center'))
  confusionMatrix(australian$V15, predict(model, australian))$overall
})
accuracies = round(info*100,2)
accuracies
```
Note:
In case of all classifiers training k-folds crossvalidation is applied.
K-nn classifier is using normalized values of attributes because of the way it calculates distance.

Secondly we will train classifiers using most important variables.
```{r, warning=FALSE, message=FALSE}
classifiers <- c('knn', 'svmLinear', 'rf', 'ctree', 'lda', 'qda', 'nb', 'svmRadial', 'LogitBoost', 'xgbTree', 'treebag', 'pls')
info <- sapply(classifiers, function (met) {
  train_control <- trainControl(method="cv", number=10)
  model <- train(V15 ~ V8 + V7 + V10 + V14 + V3, method=met, data=australian, trControl=train_control, preProcess=c('scale', 'center'))
  confusionMatrix(australian$V15, predict(model, australian))$overall
})
accuracies = round(info*100,2)
accuracies
```

From the accuracy (and kappa) summary tables we conclude that the best classifier is Bagged CART (treebag) closely followed by the Random Forest achieving 99% accuracy and 98% accuracy respectively when using all variables in training and marginally less when using five most important variables. The worst classifier in case of both feature selections according to accuracy tables is QDA with 81% when using all features and 76% when using most important ones.
For the rest of classifiers we can achieve 85-88% accuracy with/without manipulating used feature set.

# AUC of classifiers
```{r, warning=FALSE, message=FALSE}
aucs <- c()
for (cl in classifiers) {
  train_control <- trainControl(method="cv", number=10)
  model <- train(V15 ~ .,  method=cl, data=australian, trControl=trainControl(classProbs=TRUE))
  prob <- predict(model, australian, type = "prob")[,2]

  fit.pred = prediction(prob, australian$V15)
  fit.perf = performance(fit.pred,"auc")
  aucs <- c(aucs, fit.perf@y.values[[1]])
}
  aucs <- as.numeric(aucs)
  names(aucs) <- classifiers
  aucs <- sort(aucs, decreasing = TRUE)
  aucs
```

AUC calculation for all classifiers confirms that the Bagged CART (treebag) and Random Forest are the best classifiers reaching 0.999 AUC value. The knn and pls have distinctively low score - 0.84 and 0.76 respectively. They along with QDA pointed out earlier are thus worst classifiers in our set.

#Conclusions
Best classifier is Bagged CART (treebag) followed closely by Random Forest (rf). With both we can achieve very high >97% performance.

Worst classifier is QDA with small accuracy <80%. K-nearest neighbor and partial least squares have notably low AUC values (<0.85) which makes them also poor.
