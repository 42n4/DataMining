---
title: 'Data Mining Project 1 part III'
author: 'Karolina Kwasiborska, Tomasz Zaremba, Ziad Al Bkhetan '
date: "25 Nov 2015"
output:
  html_document:
    toc: yes
---

# Introduction

in this project we will use different classifiers
  <br>1: k-nearest neighbors
  <br>2: SVM
  <br>3: Decision Tree
  <br>4: Random Forests
  <br>5: Naive Bays
  <br>6: LDA
  <br>7: QDA
  <br>8: Stochastic Gradient Boosting (gbm)
  <br>9: Boosted Logistic Regression 
  <br>10: Neural Network
  <br>11: Partial Least Squares (kernelpls)
  <br>12: Robust Regularized Linear Discriminant Analysis (rrlda)
  

<br>then we will evaluate all these classifiers using:
  <br>1: ROC Curve
  <br>2: Area Under ROC curve
  <br>3: Accurecy using confusion Matrix


#Data Loading And Preparation
Starting with data loading into the variable australianDataSet using R script
<br>Data Link is http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat
<br>Target attribute "A15" is categorized using factor function
<br>The dataset is normalized for K-nearest neighbors classifier
<br>The dataset is categorized into equal size subranges for Naive Bays classifier
<br> and also in this phase we will divide the dataset into two different sets the first one for classifier training while the second is for the classifier testing.
<br> * partion percentage is 80:20
<br> * target attribute to maintain good distribution "A15"

```{r , cache=FALSE, warning=FALSE, message=FALSE}
library(mlbench)
library(caret)
library(randomForest)
library(rpart)
library(e1071)
library(MASS)
library(gbm)
library(Hmisc)
library(ROCR)
library(pls)
library(rrcov)
library(klaR)

australianDataSet = read.table("D:\\MSc Computer science and informatics\\3rd semester\\Data Mining - Advances\\Project1\\australian.dat",
        sep=" ",header=F,col.names=c("A1", "A2", "A3", "A4", 
                "A5","A6", "A7", "A8", "A9","A10",
                "A11", "A12", "A13","A14", "A15"),
        fill=FALSE,strip.white=T)

originalaustralianDataSet <-australianDataSet
australianDataSet$A15 <- factor(ifelse(australianDataSet$A15 == 0, "One", "Zero"))
#partitioning
indxTrain <- createDataPartition(y = australianDataSet$A15, p = 0.8)
australianDataSetTrain<- australianDataSet[indxTrain$Resample1,]
australianDataSetTest <- australianDataSet[-indxTrain$Resample1,]
# Data normalization
normalize <- function(x) {
  scale(x)
}

australianDataSetNormalized <- as.data.frame(lapply(australianDataSet[, 1:14], normalize))
australianDataSetNormalized$A15 <-australianDataSet$A15
# Training and Testing Datasets
indxTrain <- createDataPartition(y = australianDataSetNormalized$A15, p = 0.8)
australianDataSetNormalizedTrain<- australianDataSetNormalized[indxTrain$Resample1,]
australianDataSetNormalizedTest <- australianDataSetNormalized[-indxTrain$Resample1,]

# Data Categorization
aust_equal_size <- originalaustralianDataSet
for (i in c(2, 3, 4, 5, 6, 7, 10, 12, 13, 14)) {
  aust_equal_size[,i]<- cut2(australianDataSet[,i], g=3)
}
aust_equal_size$A15 <- factor(ifelse(aust_equal_size$A15 == 0, "One", "Zero")) 
aust_equal_size$A1 <-factor(aust_equal_size$A1)
aust_equal_size$A8 <-factor(aust_equal_size$A8)
aust_equal_size$A9 <-factor(aust_equal_size$A9)
aust_equal_size$A11 <-factor(aust_equal_size$A11)
# Partitioning 
indxTrain <- createDataPartition(y = aust_equal_size$A15, p = 0.8)
aust_equal_sizeTrain<- aust_equal_size[indxTrain$Resample1,]
aust_equal_sizeTest <- aust_equal_size[-indxTrain$Resample1,]

```

# Features Importance
Using Random forest library, we can find the important variables which describe the dataset, then we will choose the suitable model to build our classifiers
<br> we will use A15 ~ A8 + A14 + A10 + A5 +A7 + A13 + A9.

```{r , cache=FALSE, warning=FALSE, message=FALSE}
allVariablesForest <- randomForest(A15 ~ ., data = australianDataSet, importance = TRUE, na.action = na.omit)
varImpPlot(allVariablesForest)
```


# Classifiers Training And Testing
We have created a daynamic scripts to apply all classifiers on the training dataset then test the classifiers on the testing dataset, if you want to use other classifiers, just you need to put their names in the classifiers list "but they shouldn't need any exeptional workflow".

```{r , warning=FALSE, message=FALSE, results="hide"}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3, classProbs=T, savePredictions = T)
classifiersCount = 12
counter = 1
outMat<-matrix(list(), nrow=classifiersCount, ncol=2)
conf_mat <-matrix(list(), nrow=classifiersCount, ncol=1)
rocMat<-matrix(list(), nrow=classifiersCount, ncol=2)

for(classfierType in c('nb', 'rf', 'rpart', 'lda', 'qda', 'gbm', 'svmRadial', 'LogitBoost', 'knn', 'nnet', 'kernelpls', 'rrlda'))
{
  if(classfierType == 'knn')
  {
    trainDS <- australianDataSetNormalizedTrain
    testDS <- australianDataSetNormalizedTest
  }
  else if(classfierType == 'nb')
      {
        trainDS <- aust_equal_sizeTrain
        testDS  <- aust_equal_sizeTest
      } else
        {
          trainDS <- australianDataSetTrain
          testDS <- australianDataSetTest
        }
  
  currModel<- train(A15 ~ A8 + A14 + A10 + A5 +A7 + A13 + A9, data=trainDS, method=classfierType, trControl=control)
  predTab <- predict(currModel, testDS)
  outMat[[counter, 1]] <- table(true = testDS$A15, predicted = predTab)
  # Accuracy
  outMat[[counter, 2]] <- sum(diag(outMat[[counter, 1]])) / sum(outMat[[counter, 1]])
  # Confusion matrix
  conf_mat[[counter, 1]] <- confusionMatrix(predTab, testDS$A15)
  # ROC and AUC
  
  predTabPro <- predict(currModel, newdata=testDS, type="prob")[,2] 
  pred <- prediction(predTabPro, testDS$A15)
  rocMat[[counter, 1]] <- performance(pred, "tpr", "fpr")
  # Area Under Curve
  rocMat[[counter, 2]] <- performance(pred,"auc")

  counter= counter + 1
}

```

# ROC (Receiver Operating Characteristics)
In this part, we will draw the ROC curves for all classifiers using the data from the previous script.
```{r , cache=FALSE, warning=FALSE, message=FALSE}
colors <- c('aquamarine', 'chartreuse', 'chocolate1', 'coral1', 'blue', 'cyan', 'darkgoldenrod1', 'brown1', 'firebrick1', 'gold', 'deeppink', 'mediumorchid1', 'palegreen')

plot(rocMat[[1, 1]], col=colors[1])
for (counter in 2:classifiersCount)
{
  plot(rocMat[[counter, 1]], col=colors[counter], add=TRUE)
}
abline(a=0, b=1)

```
<br>Legend:<br>
```{r }
clas<-c("Naive Bays", "Random Forests", "Descion Tree", "LDA", "QDA", "GBM", "SVM", "LogitBoost", "KNN", "Neural Network", "Kernelpls", "rrlda")
colors <- c('aquamarine', 'chartreuse', 'chocolate1', 'coral1', 'blue', 'cyan', 'darkgoldenrod1', 'brown1', 'firebrick1', 'gold', 'deeppink', 'mediumorchid1', 'palegreen')

for(counter in 1:classifiersCount)
{
  print (paste("Classifier : " , clas[counter], " Color:", colors[counter], sep = "  "))
}

```

# Results
```{r }
vals <- rep(0.00, times = 24)
for(counter in 1:classifiersCount)
{
  vals[2 * counter - 1] = outMat[[counter, 2]]
  vals[2 * counter ] = rocMat[[counter, 2]]@y.values[[1]]
}

mesTab <- matrix(vals, ncol=2, nrow = 12, byrow = TRUE)
colnames(mesTab) <- c("Accuracy",  "AUC")
rownames(mesTab) <- c("Naive Bays", "Random Forests", "Descion Tree", "LDA", "QDA", "GBM", "SVM", "LogitBoost", "KNN", "Neural Network", "Kernelpls", "rrlda")
mesTab
```
# Conclusion
The best features when it comes to their importance in classification process according to the plots are: A8, A14, A10, A5, A7, A13 and A9. 
<br>The least important features are: A1, A11, A12, A4 and A6.
<br>Classification results were evaluated using three methods: accuracy, ROC curves and Area Under Curve. 
<br>According to them the best results were obtained for GBM, Naive Bayes and Random Forest. The accuracy was very high for them and so was the true positive rate. Most of the classifiers in general did very well, except rrlda, where many customers were identified as credit-worthy even though they shouldn't have been.
<br> We have a lot of different models to try, using different features set, different classifiers, and different parameters for the classifiers, but in this report we are requested to prepare 3 pages, so we decided to use as much classifiers as possible, using the most important variables, and with special data preprocessing for some classifiers.
